from __future__ import annotations

import os
import time
from datetime import datetime, timedelta, timezone

from sqlmodel import Session, select

from db import engine
from bucket_import import sync_bucket_once, get_bucket_config, BucketConfig
from analysis_engine import enqueue_missing_analyses
from app_config import is_auto_analysis_enabled, get_app_config
from models import BucketObject, CronState, AIAnalysisJob
from notify import send_feishu_webhook, get_feishu_webhook_url
from qc_checker import get_conversations_eligible_for_qc


_TZ_SH = timezone(timedelta(hours=8))


def _now() -> datetime:
    return datetime.now(tz=_TZ_SH)


def _parse_hhmm(v: str, default: str = "10:15") -> tuple[int, int]:
    s = (v or default).strip()
    if ":" not in s:
        return (10, 15)
    a, b = s.split(":", 1)
    try:
        return (max(0, min(23, int(a))), max(0, min(59, int(b))))
    except Exception:
        return (10, 15)


def _date_prefix(base_prefix: str, date_str: str) -> str:
    # ä¹è¨€ç›®å½•ç»“æ„ï¼š/leyan/YYYY-MM-DD/xxx.jsonl.gz
    bp = (base_prefix or "").strip()
    bp = bp.strip("/")
    if not bp:
        return f"{date_str}/"
    return f"{bp}/{date_str}/"


def _get_state(session: Session, name: str) -> CronState:
    st = session.exec(select(CronState).where(CronState.name == name)).first()
    if st:
        return st
    st = CronState(name=name, state={})
    session.add(st)
    session.commit()
    session.refresh(st)
    return st


def _set_state(session: Session, st: CronState, state: dict) -> None:
    st.state = state
    st.updated_at = datetime.utcnow()
    session.add(st)
    session.commit()


def _should_run_hourly_qc_check(s: dict) -> bool:
    """æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿è¡Œæ¯å°æ—¶çš„è‡ªåŠ¨è´¨æ£€æ£€æŸ¥."""
    last_qc_check = s.get("last_qc_check_at")
    if not last_qc_check:
        return True
    try:
        last_check_dt = datetime.fromisoformat(str(last_qc_check))
        if last_check_dt.tzinfo is None:
            last_check_dt = last_check_dt.replace(tzinfo=_TZ_SH)
        now = _now()
        # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡
        return (now - last_check_dt).total_seconds() >= 3600
    except Exception:
        return True


def _run_auto_qc_check(session: Session, state_dict: dict, log_keep: int = 800) -> dict:
    """æ‰§è¡Œè‡ªåŠ¨è´¨æ£€æ£€æŸ¥ï¼šæŸ¥æ‰¾ç¬¦åˆæ¡ä»¶çš„å¯¹è¯å¹¶å…¥é˜Ÿ.
    
    è¿”å›æ›´æ–°åçš„ state_dict
    """
    try:
        # è·å–ç¬¦åˆæ¡ä»¶çš„å¯¹è¯ï¼ˆ>5æ¡æ¶ˆæ¯ä¸”æ‰€æœ‰å®¢æœå·²ç»‘å®šï¼‰
        eligible = get_conversations_eligible_for_qc(session, min_messages=5, limit=None)
        
        if not eligible:
            state_dict = _append_log_state(
                state_dict,
                "INFO",
                f"è‡ªåŠ¨è´¨æ£€æ£€æŸ¥ï¼šæ— ç¬¦åˆæ¡ä»¶çš„å¯¹è¯",
                keep=log_keep
            )
            state_dict["last_qc_check_at"] = _now().isoformat()
            return state_dict
        
        # ä¸ºç¬¦åˆæ¡ä»¶çš„å¯¹è¯åˆ›å»ºåˆ†æä»»åŠ¡
        enqueued = 0
        for item in eligible:
            cid = item["conversation_id"]
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ä»»åŠ¡ï¼ˆpending æˆ– runningï¼‰
            existing = session.exec(
                select(AIAnalysisJob).where(
                    AIAnalysisJob.conversation_id == cid,
                    AIAnalysisJob.status.in_(["pending", "running"])
                )
            ).first()
            
            if not existing:
                session.add(AIAnalysisJob(conversation_id=cid))
                enqueued += 1
        
        if enqueued > 0:
            session.commit()
            state_dict = _append_log_state(
                state_dict,
                "INFO",
                f"è‡ªåŠ¨è´¨æ£€æ£€æŸ¥ï¼šå‘ç° {len(eligible)} ä¸ªç¬¦åˆæ¡ä»¶çš„å¯¹è¯ï¼Œå…¥é˜Ÿ {enqueued} ä¸ªæ–°ä»»åŠ¡",
                keep=log_keep
            )
            # å‘é€é£ä¹¦é€šçŸ¥
            try:
                _notify(
                    session,
                    "âœ… è‡ªåŠ¨è´¨æ£€å·²å¯åŠ¨",
                    f"å‘ç° {len(eligible)} ä¸ªç¬¦åˆè´¨æ£€æ¡ä»¶çš„å¯¹è¯\nå·²ä¸º {enqueued} ä¸ªå¯¹è¯åˆ›å»ºAIè´¨æ£€ä»»åŠ¡\n\næ¡ä»¶ï¼šæ¶ˆæ¯æ•°>5 ä¸” æ‰€æœ‰å®¢æœå·²ç»‘å®š"
                )
            except Exception:
                pass
        else:
            state_dict = _append_log_state(
                state_dict,
                "INFO",
                f"è‡ªåŠ¨è´¨æ£€æ£€æŸ¥ï¼šå‘ç° {len(eligible)} ä¸ªç¬¦åˆæ¡ä»¶çš„å¯¹è¯ï¼Œä½†éƒ½å·²æœ‰ä»»åŠ¡åœ¨é˜Ÿåˆ—ä¸­",
                keep=log_keep
            )
        
        state_dict["last_qc_check_at"] = _now().isoformat()
        state_dict["last_qc_check_eligible"] = len(eligible)
        state_dict["last_qc_check_enqueued"] = enqueued
        
    except Exception as e:
        state_dict = _append_log_state(
            state_dict,
            "ERROR",
            f"è‡ªåŠ¨è´¨æ£€æ£€æŸ¥å¤±è´¥ï¼š{str(e)}",
            keep=log_keep
        )
        state_dict["last_qc_check_at"] = _now().isoformat()
    
    return state_dict


def _is_date_imported(session: Session, bucket: str, date_prefix: str) -> bool:
    # any successfully imported object under that prefix counts as "fetched"
    row = session.exec(
        select(BucketObject).where(
            BucketObject.bucket == bucket,
            BucketObject.key.like(f"{date_prefix}%"),
            BucketObject.status == "imported",
        )
    ).first()
    return row is not None


def _notify(session: Session, title: str, text: str) -> None:
    url = get_feishu_webhook_url(session)
    if not url:
        return
    send_feishu_webhook(url, title=title, text=text)


def _append_log_state(s: dict, level: str, msg: str, keep: int = 800) -> dict:
    """Append one log line into a cron state dict (pure function)."""
    s = dict(s or {})
    logs = list(s.get("logs") or [])
    logs.append({"ts": _now().isoformat(), "level": (level or "INFO").upper(), "msg": str(msg or "")})
    if len(logs) > keep:
        logs = logs[-keep:]
    s["logs"] = logs
    return s


def _attempt_fetch_one(session: Session, *, source: str, cfg: BucketConfig, platform: str, date_str: str) -> dict:
    date_pref = _date_prefix(cfg.prefix, date_str)
    # If already imported, we treat as success and avoid re-scan
    if _is_date_imported(session, cfg.bucket, date_pref):
        return {"ok": True, "already": True, "date": date_str, "prefix": date_pref}

    res = sync_bucket_once(session, cfg=cfg, prefix=date_pref, platform=platform, imported_by_user_id=None, create_jobs_if_missing=False)
    res["date"] = date_str
    res["date_prefix"] = date_pref
    res["source"] = source
    res["platform"] = platform
    return res


def main() -> None:
    state_name = "bucket_poll_v1"
    cron_sleep = 10  # default if session fails

    while True:
        try:
            with Session(engine) as session:
                cfg_row = get_app_config(session)
                # runtime settings from AppConfig
                (hh, mm) = _parse_hhmm(cfg_row.bucket_daily_check_time or "10:15", default="10:15")
                retry_minutes = max(5, int(getattr(cfg_row, "bucket_retry_interval_minutes", 60) or 60))
                log_keep = max(100, min(5000, int(getattr(cfg_row, "bucket_log_keep", 800) or 800)))
                cron_sleep = max(10, int(getattr(cfg_row, "cron_interval_seconds", 600) or 600))

                # which sources to import (from AppConfig / settings UI)
                sources: list[tuple[str, str]] = []  # (SOURCE_PREFIX, platform)
                if bool(getattr(cfg_row, "taobao_bucket_import_enabled", True)):
                    sources.append(("TAOBAO", "taobao"))
                if bool(getattr(cfg_row, "douyin_bucket_import_enabled", False)):
                    sources.append(("DOUYIN", "douyin"))

                st = _get_state(session, state_name)
                s = dict(st.state or {})

                now = _now()
                yesterday = (now - timedelta(days=1)).date().isoformat()

                pending_date = s.get("pending_date") or ""
                next_attempt_at = s.get("next_attempt_at")  # iso str
                last_daily_mark = s.get("last_daily_mark") or ""  # YYYY-MM-DD of the "yesterday" chosen that morning

                # if we have no pending date, decide whether it's time to create one
                today_run = now.replace(hour=hh, minute=mm, second=0, microsecond=0)
                if not pending_date:
                    # allow disabling cron polling from UI
                    if not bool(getattr(cfg_row, "bucket_fetch_enabled", True)):
                        # sleep a bit; still keep process alive (manual triggers happen in web app)
                        time.sleep(10)
                        continue

                    if now >= today_run and last_daily_mark != yesterday:
                        # create a new pending date (yesterday)
                        pending_date = yesterday
                        s["pending_date"] = pending_date
                        s["last_daily_mark"] = yesterday
                        # immediate attempt
                        s["next_attempt_at"] = now.isoformat()
                        next_attempt_at = s["next_attempt_at"]
                        s = _append_log_state(s, "INFO", f"å¼€å§‹è½®å·¡ï¼šç›®æ ‡æ—¥æœŸ {pending_date}ï¼Œæ¥æº {', '.join([x[0] for x in sources]) or 'ï¼ˆæœªå¯ç”¨ï¼‰'}", keep=log_keep)
                        _set_state(session, st, s)
                        _notify(
                            session,
                            "èŠå¤©è®°å½•æŠ“å–ï¼šå¼€å§‹è½®å·¡",
                            f"ç›®æ ‡æ—¥æœŸï¼š{pending_date}\næ¥æºï¼š{', '.join([x[0] for x in sources]) or 'ï¼ˆæœªå¯ç”¨ï¼‰'}",
                        )

                # if still no pending_date (e.g. before daily time), sleep until daily time
                if not pending_date:
                    sleep_to = today_run if now < today_run else (today_run + timedelta(days=1))
                    delta = max(5, int((sleep_to - now).total_seconds()))
                    time.sleep(min(delta, 300))  # wake up at least every 5min
                    continue

                # respect next_attempt_at if set
                if next_attempt_at:
                    try:
                        nta = datetime.fromisoformat(str(next_attempt_at))
                        if nta.tzinfo is None:
                            nta = nta.replace(tzinfo=_TZ_SH)
                    except Exception:
                        nta = now
                else:
                    nta = now

                if now < nta:
                    time.sleep(min(int((nta - now).total_seconds()), 300))
                    continue

                # run import attempts
                if not sources:
                    # nothing enabled; back off 1h
                    s = _append_log_state(s, "WARN", "æœªå¯ç”¨ä»»ä½•æ¥æºï¼ˆæ·˜å®/æŠ–éŸ³ï¼‰ï¼Œ1å°æ—¶åå†è¯•", keep=log_keep)
                    s["next_attempt_at"] = (_now() + timedelta(hours=1)).isoformat()
                    _set_state(session, st, s)
                    time.sleep(60)
                    continue

                all_ok = True
                any_imported = False
                any_seen = False
                summaries = []

                for (src, platform) in sources:
                    cfg = get_bucket_config(src, session)
                    if not cfg.bucket:
                        all_ok = False
                        summaries.append(f"{src}: bucket æœªé…ç½®")
                        continue

                    try:
                        res = _attempt_fetch_one(session, source=src, cfg=cfg, platform=platform, date_str=pending_date)

                        # Notify when new (previously unseen) unbound external agent accounts appear.
                        try:
                            unbound = [str(x) for x in (res.get("unbound_agent_accounts") or []) if str(x).strip()]
                            unbound_nicks = res.get("unbound_agent_nicks") or {}
                            if unbound:
                                known_map = dict(s.get("known_unbound_agent_accounts") or {})
                                known_list = list(known_map.get(platform) or [])
                                known_set = set([str(x) for x in known_list if str(x).strip()])
                                new_set = set(unbound) - known_set
                                if new_set:
                                    # record first, so even if webhook fails we won't spam every retry
                                    known_set = known_set.union(new_set)
                                    known_map[platform] = sorted(list(known_set))
                                    s["known_unbound_agent_accounts"] = known_map
                                    # æ„å»ºå¸¦æ˜µç§°çš„é€šçŸ¥æ¶ˆæ¯
                                    msg_lines = []
                                    for acc in sorted(list(new_set)):
                                        nick = unbound_nicks.get(acc, "")
                                        if nick:
                                            msg_lines.append(f"â€¢ {acc} (æ˜µç§°: {nick})")
                                        else:
                                            msg_lines.append(f"â€¢ {acc}")
                                    _notify(
                                        session,
                                        "âš ï¸ å‘ç°æœªç»‘å®šå®¢æœè´¦å·",
                                        f"å¹³å°ï¼š{platform}\næ—¥æœŸï¼š{pending_date}\n\næœªç»‘å®šè´¦å·ï¼ˆæ–°å¢ï¼‰ï¼š\n" + "\n".join(msg_lines) + "\n\nè¯·åˆ°ã€è®¾ç½® > å®¢æœè´¦å·ç»‘å®šã€‘é¡µé¢è¿›è¡Œé…ç½®ã€‚",
                                    )
                        except Exception:
                            pass
                        if res.get("already"):
                            summaries.append(f"{src}: å·²æŠ“å–è¿‡ï¼ˆ{pending_date}ï¼‰")
                            any_imported = True
                            continue

                        seen = int(res.get("seen") or 0)
                        imported = int(res.get("imported") or 0)
                        errors = int(res.get("errors") or 0)
                        any_seen = any_seen or (seen > 0)
                        any_imported = any_imported or (imported > 0)

                        if seen <= 0:
                            all_ok = False
                            summaries.append(f"{src}: æœªå‘ç°æ–‡ä»¶ï¼ˆ{pending_date}ï¼‰")
                        elif errors > 0 and imported <= 0:
                            all_ok = False
                            summaries.append(f"{src}: å‘ç° {seen} ä¸ªæ–‡ä»¶ï¼Œä½†å¯¼å…¥å¤±è´¥ {errors} æ¬¡")
                        else:
                            summaries.append(f"{src}: å‘ç° {seen} ä¸ªæ–‡ä»¶ï¼Œå¯¼å…¥æˆåŠŸ {imported}ï¼Œå¤±è´¥ {errors}")

                    except Exception as e:
                        all_ok = False
                        summaries.append(f"{src}: å¼‚å¸¸ {e}")

                # notify + schedule next
                if any_imported:
                    s = _append_log_state(s, "INFO", "æŠ“å–æˆåŠŸï¼š" + " | ".join(summaries), keep=log_keep)
                    _notify(session, "ğŸ“¥ èŠå¤©è®°å½•æŠ“å–ï¼šæˆåŠŸ", "\n".join(summaries))
                    # clear pending
                    s["pending_date"] = ""
                    s["next_attempt_at"] = ""
                    _set_state(session, st, s)
                    
                    # å¯¼å…¥æˆåŠŸåï¼Œç«‹å³æ£€æŸ¥æ˜¯å¦æœ‰ç¬¦åˆæ¡ä»¶çš„å¯¹è¯å¯ä»¥è´¨æ£€
                    # ï¼ˆä¸éœ€è¦ç­‰å¾…ä¸‹ä¸€ä¸ªå°æ—¶æ£€æŸ¥ï¼‰
                    try:
                        st = _get_state(session, state_name)
                        s = dict(st.state or {})
                        s = _run_auto_qc_check(session, s, log_keep=log_keep)
                        _set_state(session, st, s)
                    except Exception:
                        # å³ä½¿æ£€æŸ¥å¤±è´¥ï¼Œä¹Ÿä¸å½±å“å¯¼å…¥æµç¨‹
                        pass
                else:
                    # missing or failed â€” retry hourly
                    s = _append_log_state(s, "WARN", "æœªæŠ“åˆ°/å¯¼å…¥å¤±è´¥ï¼ˆå°†é‡è¯•ï¼‰ï¼š" + " | ".join(summaries), keep=log_keep)
                    _notify(session, "èŠå¤©è®°å½•æŠ“å–ï¼šæœªæŠ“åˆ°ï¼ˆå°†æ¯å°æ—¶é‡è¯•ï¼‰", "\n".join(summaries))
                    s["next_attempt_at"] = (_now() + timedelta(minutes=retry_minutes)).isoformat()
                    _set_state(session, st, s)

                # enqueue analysis if enabled
                if bool(getattr(cfg_row, "enable_enqueue_analysis", True)):
                    if is_auto_analysis_enabled(session):
                        enqueue_missing_analyses(session)
                
                # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡ï¼šç¬¦åˆæ¡ä»¶çš„å¯¹è¯è‡ªåŠ¨è§¦å‘AIè´¨æ£€
                # æ¡ä»¶ï¼š>5æ¶ˆæ¯ ä¸” æ‰€æœ‰å®¢æœå·²ç»‘å®š
                st = _get_state(session, state_name)
                s = dict(st.state or {})
                if _should_run_hourly_qc_check(s):
                    s = _run_auto_qc_check(session, s, log_keep=log_keep)
                    _set_state(session, st, s)

        except Exception:
            # best-effort cron
            pass

        time.sleep(cron_sleep)


if __name__ == "__main__":
    main()
